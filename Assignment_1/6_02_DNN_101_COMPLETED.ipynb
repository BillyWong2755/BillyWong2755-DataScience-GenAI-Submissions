{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BillyWong2755/BillyWong2755-DataScience-GenAI-Submissions/blob/main/Assignment_1/6_02_DNN_101_COMPLETED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "7c6672b5-473b-4c70-8a68-78f45c2398a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "de2070e0-de5e-4257-b6f0-97830b8b3e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "63db548b-c291-485b-8146-cd036d41ceb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "580778b7-dedd-4fad-8659-96ab4a68e84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 27792.6836\n",
            "Epoch [10/100], Loss: 29229.0\n",
            "Epoch [10/100], Loss: 33445.2578\n",
            "Epoch [10/100], Loss: 34277.4727\n",
            "Epoch [10/100], Loss: 30698.2168\n",
            "Epoch [10/100], Loss: 29240.0117\n",
            "Epoch [10/100], Loss: 23086.3223\n",
            "Epoch [10/100], Loss: 41652.0742\n",
            "Epoch [20/100], Loss: 36997.7031\n",
            "Epoch [20/100], Loss: 31123.291\n",
            "Epoch [20/100], Loss: 37788.7305\n",
            "Epoch [20/100], Loss: 24601.3066\n",
            "Epoch [20/100], Loss: 25494.0449\n",
            "Epoch [20/100], Loss: 26740.2871\n",
            "Epoch [20/100], Loss: 23668.1992\n",
            "Epoch [20/100], Loss: 49332.1055\n",
            "Epoch [30/100], Loss: 23400.6035\n",
            "Epoch [30/100], Loss: 28949.0312\n",
            "Epoch [30/100], Loss: 28942.0488\n",
            "Epoch [30/100], Loss: 33954.5898\n",
            "Epoch [30/100], Loss: 34329.9961\n",
            "Epoch [30/100], Loss: 27848.959\n",
            "Epoch [30/100], Loss: 27293.4375\n",
            "Epoch [30/100], Loss: 51907.5312\n",
            "Epoch [40/100], Loss: 29354.3652\n",
            "Epoch [40/100], Loss: 26514.3477\n",
            "Epoch [40/100], Loss: 39807.8398\n",
            "Epoch [40/100], Loss: 24769.0312\n",
            "Epoch [40/100], Loss: 31261.3535\n",
            "Epoch [40/100], Loss: 24624.8711\n",
            "Epoch [40/100], Loss: 24942.2285\n",
            "Epoch [40/100], Loss: 66960.8281\n",
            "Epoch [50/100], Loss: 29002.2422\n",
            "Epoch [50/100], Loss: 27885.4297\n",
            "Epoch [50/100], Loss: 30575.2891\n",
            "Epoch [50/100], Loss: 26601.6426\n",
            "Epoch [50/100], Loss: 25302.8965\n",
            "Epoch [50/100], Loss: 25023.1543\n",
            "Epoch [50/100], Loss: 36119.625\n",
            "Epoch [50/100], Loss: 15989.5762\n",
            "Epoch [60/100], Loss: 20564.9512\n",
            "Epoch [60/100], Loss: 30450.584\n",
            "Epoch [60/100], Loss: 29662.8125\n",
            "Epoch [60/100], Loss: 30478.4102\n",
            "Epoch [60/100], Loss: 27901.4746\n",
            "Epoch [60/100], Loss: 23868.9395\n",
            "Epoch [60/100], Loss: 31848.9941\n",
            "Epoch [60/100], Loss: 27383.0859\n",
            "Epoch [70/100], Loss: 22162.6055\n",
            "Epoch [70/100], Loss: 30928.8848\n",
            "Epoch [70/100], Loss: 28300.1094\n",
            "Epoch [70/100], Loss: 27099.1836\n",
            "Epoch [70/100], Loss: 22190.168\n",
            "Epoch [70/100], Loss: 25000.084\n",
            "Epoch [70/100], Loss: 28157.6445\n",
            "Epoch [70/100], Loss: 94185.9922\n",
            "Epoch [80/100], Loss: 24935.1738\n",
            "Epoch [80/100], Loss: 22045.0488\n",
            "Epoch [80/100], Loss: 32214.5898\n",
            "Epoch [80/100], Loss: 22693.625\n",
            "Epoch [80/100], Loss: 26515.7148\n",
            "Epoch [80/100], Loss: 25106.7285\n",
            "Epoch [80/100], Loss: 25922.1992\n",
            "Epoch [80/100], Loss: 23149.0684\n",
            "Epoch [90/100], Loss: 26538.166\n",
            "Epoch [90/100], Loss: 24762.5762\n",
            "Epoch [90/100], Loss: 25060.8945\n",
            "Epoch [90/100], Loss: 21411.6621\n",
            "Epoch [90/100], Loss: 18221.1367\n",
            "Epoch [90/100], Loss: 22946.9141\n",
            "Epoch [90/100], Loss: 29453.1094\n",
            "Epoch [90/100], Loss: 35444.7031\n",
            "Epoch [100/100], Loss: 18367.6328\n",
            "Epoch [100/100], Loss: 19565.3926\n",
            "Epoch [100/100], Loss: 25856.5449\n",
            "Epoch [100/100], Loss: 24234.4648\n",
            "Epoch [100/100], Loss: 17946.4238\n",
            "Epoch [100/100], Loss: 28564.8262\n",
            "Epoch [100/100], Loss: 22947.1035\n",
            "Epoch [100/100], Loss: 24181.4883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "cf99cede-080f-4a93-fd8c-ffec8c37075b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 19474.7724609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "f2a4b913-56f8-4b32-bae3-75ee7f1a2dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   26.645344   219.0\n",
              "1   25.103045    70.0\n",
              "2   26.455063   202.0\n",
              "3   32.560081   230.0\n",
              "4   25.477730   111.0\n",
              "..        ...     ...\n",
              "84  22.497879   153.0\n",
              "85  20.658676    98.0\n",
              "86  18.699120    37.0\n",
              "87  19.472189    63.0\n",
              "88  22.983818   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95a6220c-4ebf-441e-80f1-dbff33b12c34\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26.645344</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25.103045</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>26.455063</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32.560081</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.477730</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>22.497879</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>20.658676</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>18.699120</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>19.472189</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>22.983818</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95a6220c-4ebf-441e-80f1-dbff33b12c34')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95a6220c-4ebf-441e-80f1-dbff33b12c34 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95a6220c-4ebf-441e-80f1-dbff33b12c34');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-82b5347e-3311-417e-9f40-d2ea7959da87\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82b5347e-3311-417e-9f40-d2ea7959da87')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-82b5347e-3311-417e-9f40-d2ea7959da87 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3ab0d4bd-1d66-4186-90ad-1c2e0c693963\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3ab0d4bd-1d66-4186-90ad-1c2e0c693963 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          25.717517852783203,\n          22.338348388671875,\n          25.943696975708008\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXERCISE #1"
      ],
      "metadata": {
        "id": "Q50LAHZ9px34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # 1000 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "id": "ha_3tgu_pU77",
        "outputId": "3e0c6cb4-e3e2-4ed4-ee8d-8ba2bf63b449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 2831.1997\n",
            "Epoch [10/1000], Loss: 4241.2793\n",
            "Epoch [10/1000], Loss: 2508.1794\n",
            "Epoch [10/1000], Loss: 3424.5417\n",
            "Epoch [10/1000], Loss: 2878.22\n",
            "Epoch [10/1000], Loss: 2453.1174\n",
            "Epoch [10/1000], Loss: 2537.0288\n",
            "Epoch [10/1000], Loss: 2320.3032\n",
            "Epoch [20/1000], Loss: 3399.8606\n",
            "Epoch [20/1000], Loss: 2994.2737\n",
            "Epoch [20/1000], Loss: 2764.938\n",
            "Epoch [20/1000], Loss: 2788.2737\n",
            "Epoch [20/1000], Loss: 3271.467\n",
            "Epoch [20/1000], Loss: 3175.3723\n",
            "Epoch [20/1000], Loss: 2555.5181\n",
            "Epoch [20/1000], Loss: 602.6763\n",
            "Epoch [30/1000], Loss: 2751.5481\n",
            "Epoch [30/1000], Loss: 2244.5798\n",
            "Epoch [30/1000], Loss: 3756.0793\n",
            "Epoch [30/1000], Loss: 3050.4077\n",
            "Epoch [30/1000], Loss: 3115.4746\n",
            "Epoch [30/1000], Loss: 2633.3606\n",
            "Epoch [30/1000], Loss: 3365.3438\n",
            "Epoch [30/1000], Loss: 1041.2585\n",
            "Epoch [40/1000], Loss: 2729.0117\n",
            "Epoch [40/1000], Loss: 2859.6768\n",
            "Epoch [40/1000], Loss: 3021.7949\n",
            "Epoch [40/1000], Loss: 2720.7773\n",
            "Epoch [40/1000], Loss: 2875.0876\n",
            "Epoch [40/1000], Loss: 2518.5581\n",
            "Epoch [40/1000], Loss: 4169.1274\n",
            "Epoch [40/1000], Loss: 980.198\n",
            "Epoch [50/1000], Loss: 2812.79\n",
            "Epoch [50/1000], Loss: 3180.6885\n",
            "Epoch [50/1000], Loss: 2788.228\n",
            "Epoch [50/1000], Loss: 3454.5662\n",
            "Epoch [50/1000], Loss: 3412.2253\n",
            "Epoch [50/1000], Loss: 2129.2585\n",
            "Epoch [50/1000], Loss: 2956.115\n",
            "Epoch [50/1000], Loss: 3365.7144\n",
            "Epoch [60/1000], Loss: 3999.1863\n",
            "Epoch [60/1000], Loss: 2254.9275\n",
            "Epoch [60/1000], Loss: 3360.8674\n",
            "Epoch [60/1000], Loss: 3792.8523\n",
            "Epoch [60/1000], Loss: 2402.2161\n",
            "Epoch [60/1000], Loss: 2741.4263\n",
            "Epoch [60/1000], Loss: 2302.1443\n",
            "Epoch [60/1000], Loss: 1123.2262\n",
            "Epoch [70/1000], Loss: 2031.4908\n",
            "Epoch [70/1000], Loss: 3151.4312\n",
            "Epoch [70/1000], Loss: 3723.8381\n",
            "Epoch [70/1000], Loss: 2892.4136\n",
            "Epoch [70/1000], Loss: 3415.7588\n",
            "Epoch [70/1000], Loss: 3376.7637\n",
            "Epoch [70/1000], Loss: 2282.7739\n",
            "Epoch [70/1000], Loss: 500.4231\n",
            "Epoch [80/1000], Loss: 2411.1719\n",
            "Epoch [80/1000], Loss: 2568.1465\n",
            "Epoch [80/1000], Loss: 2822.6382\n",
            "Epoch [80/1000], Loss: 2917.2617\n",
            "Epoch [80/1000], Loss: 2952.7124\n",
            "Epoch [80/1000], Loss: 3661.4788\n",
            "Epoch [80/1000], Loss: 3292.7349\n",
            "Epoch [80/1000], Loss: 4345.0146\n",
            "Epoch [90/1000], Loss: 3065.5261\n",
            "Epoch [90/1000], Loss: 3562.3271\n",
            "Epoch [90/1000], Loss: 2821.6106\n",
            "Epoch [90/1000], Loss: 2770.5662\n",
            "Epoch [90/1000], Loss: 3451.0669\n",
            "Epoch [90/1000], Loss: 2139.4236\n",
            "Epoch [90/1000], Loss: 2900.8918\n",
            "Epoch [90/1000], Loss: 2571.1748\n",
            "Epoch [100/1000], Loss: 2920.4546\n",
            "Epoch [100/1000], Loss: 2360.5244\n",
            "Epoch [100/1000], Loss: 2756.8787\n",
            "Epoch [100/1000], Loss: 3341.6111\n",
            "Epoch [100/1000], Loss: 3697.2292\n",
            "Epoch [100/1000], Loss: 2732.8474\n",
            "Epoch [100/1000], Loss: 2758.3064\n",
            "Epoch [100/1000], Loss: 4669.4302\n",
            "Epoch [110/1000], Loss: 3298.2974\n",
            "Epoch [110/1000], Loss: 2954.7725\n",
            "Epoch [110/1000], Loss: 2727.2236\n",
            "Epoch [110/1000], Loss: 2411.9592\n",
            "Epoch [110/1000], Loss: 2496.2605\n",
            "Epoch [110/1000], Loss: 3318.8486\n",
            "Epoch [110/1000], Loss: 3351.8599\n",
            "Epoch [110/1000], Loss: 4605.5894\n",
            "Epoch [120/1000], Loss: 3173.6948\n",
            "Epoch [120/1000], Loss: 2104.9783\n",
            "Epoch [120/1000], Loss: 3470.9287\n",
            "Epoch [120/1000], Loss: 3160.5574\n",
            "Epoch [120/1000], Loss: 2342.2122\n",
            "Epoch [120/1000], Loss: 3302.968\n",
            "Epoch [120/1000], Loss: 3191.8708\n",
            "Epoch [120/1000], Loss: 1250.1448\n",
            "Epoch [130/1000], Loss: 3117.1755\n",
            "Epoch [130/1000], Loss: 2644.2087\n",
            "Epoch [130/1000], Loss: 3669.28\n",
            "Epoch [130/1000], Loss: 2775.4556\n",
            "Epoch [130/1000], Loss: 2150.073\n",
            "Epoch [130/1000], Loss: 2850.9377\n",
            "Epoch [130/1000], Loss: 3211.7393\n",
            "Epoch [130/1000], Loss: 6458.4131\n",
            "Epoch [140/1000], Loss: 1452.1412\n",
            "Epoch [140/1000], Loss: 3197.2539\n",
            "Epoch [140/1000], Loss: 2993.0051\n",
            "Epoch [140/1000], Loss: 3415.7041\n",
            "Epoch [140/1000], Loss: 3789.4465\n",
            "Epoch [140/1000], Loss: 2519.2646\n",
            "Epoch [140/1000], Loss: 3389.0981\n",
            "Epoch [140/1000], Loss: 726.6981\n",
            "Epoch [150/1000], Loss: 3121.7783\n",
            "Epoch [150/1000], Loss: 3433.2209\n",
            "Epoch [150/1000], Loss: 2538.5203\n",
            "Epoch [150/1000], Loss: 2486.5312\n",
            "Epoch [150/1000], Loss: 3775.7188\n",
            "Epoch [150/1000], Loss: 2746.4424\n",
            "Epoch [150/1000], Loss: 2388.748\n",
            "Epoch [150/1000], Loss: 5006.6748\n",
            "Epoch [160/1000], Loss: 3546.6475\n",
            "Epoch [160/1000], Loss: 3506.5654\n",
            "Epoch [160/1000], Loss: 2543.4744\n",
            "Epoch [160/1000], Loss: 3000.9143\n",
            "Epoch [160/1000], Loss: 2662.863\n",
            "Epoch [160/1000], Loss: 2062.8838\n",
            "Epoch [160/1000], Loss: 2799.4712\n",
            "Epoch [160/1000], Loss: 10845.2305\n",
            "Epoch [170/1000], Loss: 3133.3562\n",
            "Epoch [170/1000], Loss: 2571.136\n",
            "Epoch [170/1000], Loss: 3438.6443\n",
            "Epoch [170/1000], Loss: 2194.2051\n",
            "Epoch [170/1000], Loss: 4145.1226\n",
            "Epoch [170/1000], Loss: 2916.6399\n",
            "Epoch [170/1000], Loss: 2249.011\n",
            "Epoch [170/1000], Loss: 2052.395\n",
            "Epoch [180/1000], Loss: 2961.7031\n",
            "Epoch [180/1000], Loss: 2911.8518\n",
            "Epoch [180/1000], Loss: 2498.5803\n",
            "Epoch [180/1000], Loss: 2798.5173\n",
            "Epoch [180/1000], Loss: 4016.8293\n",
            "Epoch [180/1000], Loss: 2944.3193\n",
            "Epoch [180/1000], Loss: 2470.9292\n",
            "Epoch [180/1000], Loss: 2574.4268\n",
            "Epoch [190/1000], Loss: 2395.541\n",
            "Epoch [190/1000], Loss: 2678.6506\n",
            "Epoch [190/1000], Loss: 3821.353\n",
            "Epoch [190/1000], Loss: 3353.4431\n",
            "Epoch [190/1000], Loss: 2187.0876\n",
            "Epoch [190/1000], Loss: 3217.4333\n",
            "Epoch [190/1000], Loss: 2943.9712\n",
            "Epoch [190/1000], Loss: 2402.2612\n",
            "Epoch [200/1000], Loss: 2530.1592\n",
            "Epoch [200/1000], Loss: 3078.7283\n",
            "Epoch [200/1000], Loss: 2315.9106\n",
            "Epoch [200/1000], Loss: 3340.0391\n",
            "Epoch [200/1000], Loss: 3037.228\n",
            "Epoch [200/1000], Loss: 3583.7224\n",
            "Epoch [200/1000], Loss: 2815.0708\n",
            "Epoch [200/1000], Loss: 674.5472\n",
            "Epoch [210/1000], Loss: 3551.415\n",
            "Epoch [210/1000], Loss: 3127.8318\n",
            "Epoch [210/1000], Loss: 3036.9375\n",
            "Epoch [210/1000], Loss: 2044.3756\n",
            "Epoch [210/1000], Loss: 2583.8865\n",
            "Epoch [210/1000], Loss: 3362.1418\n",
            "Epoch [210/1000], Loss: 3011.5403\n",
            "Epoch [210/1000], Loss: 73.8719\n",
            "Epoch [220/1000], Loss: 3699.4211\n",
            "Epoch [220/1000], Loss: 2923.99\n",
            "Epoch [220/1000], Loss: 2982.3105\n",
            "Epoch [220/1000], Loss: 3181.0134\n",
            "Epoch [220/1000], Loss: 2395.6978\n",
            "Epoch [220/1000], Loss: 2926.4753\n",
            "Epoch [220/1000], Loss: 2498.6384\n",
            "Epoch [220/1000], Loss: 1668.7334\n",
            "Epoch [230/1000], Loss: 3095.4661\n",
            "Epoch [230/1000], Loss: 3187.7192\n",
            "Epoch [230/1000], Loss: 3356.6189\n",
            "Epoch [230/1000], Loss: 2627.3049\n",
            "Epoch [230/1000], Loss: 3012.3987\n",
            "Epoch [230/1000], Loss: 2669.9668\n",
            "Epoch [230/1000], Loss: 2494.6138\n",
            "Epoch [230/1000], Loss: 4459.0791\n",
            "Epoch [240/1000], Loss: 2786.5613\n",
            "Epoch [240/1000], Loss: 2944.6787\n",
            "Epoch [240/1000], Loss: 3738.1406\n",
            "Epoch [240/1000], Loss: 3583.9597\n",
            "Epoch [240/1000], Loss: 2443.6121\n",
            "Epoch [240/1000], Loss: 2733.4749\n",
            "Epoch [240/1000], Loss: 2321.5188\n",
            "Epoch [240/1000], Loss: 2451.8018\n",
            "Epoch [250/1000], Loss: 2584.3906\n",
            "Epoch [250/1000], Loss: 2925.7986\n",
            "Epoch [250/1000], Loss: 2571.781\n",
            "Epoch [250/1000], Loss: 1990.7156\n",
            "Epoch [250/1000], Loss: 4093.52\n",
            "Epoch [250/1000], Loss: 2663.9978\n",
            "Epoch [250/1000], Loss: 3563.4165\n",
            "Epoch [250/1000], Loss: 4981.6074\n",
            "Epoch [260/1000], Loss: 2547.9109\n",
            "Epoch [260/1000], Loss: 3383.3049\n",
            "Epoch [260/1000], Loss: 2962.0793\n",
            "Epoch [260/1000], Loss: 2467.978\n",
            "Epoch [260/1000], Loss: 3306.7583\n",
            "Epoch [260/1000], Loss: 2680.0715\n",
            "Epoch [260/1000], Loss: 3198.991\n",
            "Epoch [260/1000], Loss: 2256.1287\n",
            "Epoch [270/1000], Loss: 3227.8708\n",
            "Epoch [270/1000], Loss: 3362.9565\n",
            "Epoch [270/1000], Loss: 3865.5095\n",
            "Epoch [270/1000], Loss: 1967.7997\n",
            "Epoch [270/1000], Loss: 3599.6501\n",
            "Epoch [270/1000], Loss: 2286.4648\n",
            "Epoch [270/1000], Loss: 2173.0469\n",
            "Epoch [270/1000], Loss: 3372.1362\n",
            "Epoch [280/1000], Loss: 2338.7493\n",
            "Epoch [280/1000], Loss: 2753.5955\n",
            "Epoch [280/1000], Loss: 3655.7458\n",
            "Epoch [280/1000], Loss: 2744.7944\n",
            "Epoch [280/1000], Loss: 1913.3199\n",
            "Epoch [280/1000], Loss: 3698.7673\n",
            "Epoch [280/1000], Loss: 3510.6606\n",
            "Epoch [280/1000], Loss: 910.5082\n",
            "Epoch [290/1000], Loss: 3264.4229\n",
            "Epoch [290/1000], Loss: 3616.9448\n",
            "Epoch [290/1000], Loss: 2802.7168\n",
            "Epoch [290/1000], Loss: 2858.4187\n",
            "Epoch [290/1000], Loss: 2208.4099\n",
            "Epoch [290/1000], Loss: 3155.189\n",
            "Epoch [290/1000], Loss: 2635.063\n",
            "Epoch [290/1000], Loss: 2114.478\n",
            "Epoch [300/1000], Loss: 2730.0049\n",
            "Epoch [300/1000], Loss: 2408.3684\n",
            "Epoch [300/1000], Loss: 3039.7114\n",
            "Epoch [300/1000], Loss: 2704.7795\n",
            "Epoch [300/1000], Loss: 2655.3193\n",
            "Epoch [300/1000], Loss: 3522.6655\n",
            "Epoch [300/1000], Loss: 3347.5068\n",
            "Epoch [300/1000], Loss: 4262.2031\n",
            "Epoch [310/1000], Loss: 1890.324\n",
            "Epoch [310/1000], Loss: 2651.3503\n",
            "Epoch [310/1000], Loss: 2886.6477\n",
            "Epoch [310/1000], Loss: 2822.4028\n",
            "Epoch [310/1000], Loss: 3783.0237\n",
            "Epoch [310/1000], Loss: 3580.9805\n",
            "Epoch [310/1000], Loss: 2807.9761\n",
            "Epoch [310/1000], Loss: 3880.9326\n",
            "Epoch [320/1000], Loss: 3585.9443\n",
            "Epoch [320/1000], Loss: 3587.3621\n",
            "Epoch [320/1000], Loss: 2643.7632\n",
            "Epoch [320/1000], Loss: 2554.3281\n",
            "Epoch [320/1000], Loss: 2880.0215\n",
            "Epoch [320/1000], Loss: 2017.5208\n",
            "Epoch [320/1000], Loss: 3137.4836\n",
            "Epoch [320/1000], Loss: 4285.6328\n",
            "Epoch [330/1000], Loss: 2397.7825\n",
            "Epoch [330/1000], Loss: 3006.9553\n",
            "Epoch [330/1000], Loss: 4047.5227\n",
            "Epoch [330/1000], Loss: 2465.968\n",
            "Epoch [330/1000], Loss: 2510.4866\n",
            "Epoch [330/1000], Loss: 3366.7505\n",
            "Epoch [330/1000], Loss: 2538.3618\n",
            "Epoch [330/1000], Loss: 5155.5474\n",
            "Epoch [340/1000], Loss: 2956.425\n",
            "Epoch [340/1000], Loss: 2426.8633\n",
            "Epoch [340/1000], Loss: 3002.1003\n",
            "Epoch [340/1000], Loss: 2462.0962\n",
            "Epoch [340/1000], Loss: 2709.2361\n",
            "Epoch [340/1000], Loss: 3981.4355\n",
            "Epoch [340/1000], Loss: 3036.7617\n",
            "Epoch [340/1000], Loss: 1074.9871\n",
            "Epoch [350/1000], Loss: 3484.2502\n",
            "Epoch [350/1000], Loss: 3324.9021\n",
            "Epoch [350/1000], Loss: 2735.3115\n",
            "Epoch [350/1000], Loss: 2451.0898\n",
            "Epoch [350/1000], Loss: 2385.5205\n",
            "Epoch [350/1000], Loss: 3549.3042\n",
            "Epoch [350/1000], Loss: 2531.1606\n",
            "Epoch [350/1000], Loss: 3047.4919\n",
            "Epoch [360/1000], Loss: 2378.5767\n",
            "Epoch [360/1000], Loss: 2649.9932\n",
            "Epoch [360/1000], Loss: 3105.9875\n",
            "Epoch [360/1000], Loss: 3035.322\n",
            "Epoch [360/1000], Loss: 2828.2358\n",
            "Epoch [360/1000], Loss: 2904.4158\n",
            "Epoch [360/1000], Loss: 3497.665\n",
            "Epoch [360/1000], Loss: 3672.5796\n",
            "Epoch [370/1000], Loss: 3703.1462\n",
            "Epoch [370/1000], Loss: 2317.6924\n",
            "Epoch [370/1000], Loss: 2738.134\n",
            "Epoch [370/1000], Loss: 2923.2495\n",
            "Epoch [370/1000], Loss: 2763.1611\n",
            "Epoch [370/1000], Loss: 3488.5542\n",
            "Epoch [370/1000], Loss: 2591.3096\n",
            "Epoch [370/1000], Loss: 1509.4204\n",
            "Epoch [380/1000], Loss: 2810.7458\n",
            "Epoch [380/1000], Loss: 3212.6692\n",
            "Epoch [380/1000], Loss: 3204.51\n",
            "Epoch [380/1000], Loss: 2305.9519\n",
            "Epoch [380/1000], Loss: 3211.8611\n",
            "Epoch [380/1000], Loss: 3050.9255\n",
            "Epoch [380/1000], Loss: 2789.5842\n",
            "Epoch [380/1000], Loss: 396.1591\n",
            "Epoch [390/1000], Loss: 2333.8015\n",
            "Epoch [390/1000], Loss: 3120.4143\n",
            "Epoch [390/1000], Loss: 3679.6436\n",
            "Epoch [390/1000], Loss: 2767.7156\n",
            "Epoch [390/1000], Loss: 3002.1887\n",
            "Epoch [390/1000], Loss: 3008.0093\n",
            "Epoch [390/1000], Loss: 2498.8677\n",
            "Epoch [390/1000], Loss: 3242.2383\n",
            "Epoch [400/1000], Loss: 3117.2878\n",
            "Epoch [400/1000], Loss: 2880.2253\n",
            "Epoch [400/1000], Loss: 2688.2595\n",
            "Epoch [400/1000], Loss: 3205.3643\n",
            "Epoch [400/1000], Loss: 2890.9836\n",
            "Epoch [400/1000], Loss: 2366.6748\n",
            "Epoch [400/1000], Loss: 3396.5183\n",
            "Epoch [400/1000], Loss: 1051.7494\n",
            "Epoch [410/1000], Loss: 3479.3062\n",
            "Epoch [410/1000], Loss: 1550.5452\n",
            "Epoch [410/1000], Loss: 2357.1013\n",
            "Epoch [410/1000], Loss: 2986.4309\n",
            "Epoch [410/1000], Loss: 3291.0073\n",
            "Epoch [410/1000], Loss: 3651.9253\n",
            "Epoch [410/1000], Loss: 3241.8931\n",
            "Epoch [410/1000], Loss: 806.725\n",
            "Epoch [420/1000], Loss: 3628.7986\n",
            "Epoch [420/1000], Loss: 3335.2712\n",
            "Epoch [420/1000], Loss: 2214.0002\n",
            "Epoch [420/1000], Loss: 2715.1616\n",
            "Epoch [420/1000], Loss: 2812.9185\n",
            "Epoch [420/1000], Loss: 2779.095\n",
            "Epoch [420/1000], Loss: 3092.5051\n",
            "Epoch [420/1000], Loss: 359.9618\n",
            "Epoch [430/1000], Loss: 3289.6575\n",
            "Epoch [430/1000], Loss: 3545.4165\n",
            "Epoch [430/1000], Loss: 3499.4937\n",
            "Epoch [430/1000], Loss: 1984.589\n",
            "Epoch [430/1000], Loss: 3544.6243\n",
            "Epoch [430/1000], Loss: 2175.7385\n",
            "Epoch [430/1000], Loss: 2402.915\n",
            "Epoch [430/1000], Loss: 2464.6572\n",
            "Epoch [440/1000], Loss: 2913.2947\n",
            "Epoch [440/1000], Loss: 3023.8867\n",
            "Epoch [440/1000], Loss: 3052.7131\n",
            "Epoch [440/1000], Loss: 2453.3264\n",
            "Epoch [440/1000], Loss: 3817.1575\n",
            "Epoch [440/1000], Loss: 2124.0762\n",
            "Epoch [440/1000], Loss: 3049.3362\n",
            "Epoch [440/1000], Loss: 2603.8728\n",
            "Epoch [450/1000], Loss: 2625.7822\n",
            "Epoch [450/1000], Loss: 2254.0327\n",
            "Epoch [450/1000], Loss: 2580.1619\n",
            "Epoch [450/1000], Loss: 2569.4045\n",
            "Epoch [450/1000], Loss: 4080.3396\n",
            "Epoch [450/1000], Loss: 3579.0549\n",
            "Epoch [450/1000], Loss: 2868.6409\n",
            "Epoch [450/1000], Loss: 556.0317\n",
            "Epoch [460/1000], Loss: 2594.1272\n",
            "Epoch [460/1000], Loss: 2476.0535\n",
            "Epoch [460/1000], Loss: 2536.9626\n",
            "Epoch [460/1000], Loss: 3180.2188\n",
            "Epoch [460/1000], Loss: 3632.5413\n",
            "Epoch [460/1000], Loss: 3077.9924\n",
            "Epoch [460/1000], Loss: 2782.9346\n",
            "Epoch [460/1000], Loss: 5087.3711\n",
            "Epoch [470/1000], Loss: 2999.7124\n",
            "Epoch [470/1000], Loss: 2872.458\n",
            "Epoch [470/1000], Loss: 2453.6118\n",
            "Epoch [470/1000], Loss: 2464.999\n",
            "Epoch [470/1000], Loss: 2637.1753\n",
            "Epoch [470/1000], Loss: 4033.0269\n",
            "Epoch [470/1000], Loss: 2885.6658\n",
            "Epoch [470/1000], Loss: 3908.1021\n",
            "Epoch [480/1000], Loss: 2641.1069\n",
            "Epoch [480/1000], Loss: 3390.0286\n",
            "Epoch [480/1000], Loss: 2509.9109\n",
            "Epoch [480/1000], Loss: 2567.1165\n",
            "Epoch [480/1000], Loss: 2822.8855\n",
            "Epoch [480/1000], Loss: 3449.9001\n",
            "Epoch [480/1000], Loss: 2730.1187\n",
            "Epoch [480/1000], Loss: 7676.3921\n",
            "Epoch [490/1000], Loss: 3708.2175\n",
            "Epoch [490/1000], Loss: 2802.52\n",
            "Epoch [490/1000], Loss: 2499.8972\n",
            "Epoch [490/1000], Loss: 2531.5046\n",
            "Epoch [490/1000], Loss: 3399.3494\n",
            "Epoch [490/1000], Loss: 2949.3728\n",
            "Epoch [490/1000], Loss: 2325.6655\n",
            "Epoch [490/1000], Loss: 5892.0249\n",
            "Epoch [500/1000], Loss: 3643.4612\n",
            "Epoch [500/1000], Loss: 1905.5452\n",
            "Epoch [500/1000], Loss: 1877.73\n",
            "Epoch [500/1000], Loss: 2277.6978\n",
            "Epoch [500/1000], Loss: 3755.0625\n",
            "Epoch [500/1000], Loss: 3385.5542\n",
            "Epoch [500/1000], Loss: 3618.542\n",
            "Epoch [500/1000], Loss: 1731.4265\n",
            "Epoch [510/1000], Loss: 3785.1458\n",
            "Epoch [510/1000], Loss: 2716.655\n",
            "Epoch [510/1000], Loss: 2668.4702\n",
            "Epoch [510/1000], Loss: 2207.3259\n",
            "Epoch [510/1000], Loss: 4054.1711\n",
            "Epoch [510/1000], Loss: 1980.223\n",
            "Epoch [510/1000], Loss: 3125.2402\n",
            "Epoch [510/1000], Loss: 554.224\n",
            "Epoch [520/1000], Loss: 2511.854\n",
            "Epoch [520/1000], Loss: 3519.0786\n",
            "Epoch [520/1000], Loss: 4259.7554\n",
            "Epoch [520/1000], Loss: 2832.1174\n",
            "Epoch [520/1000], Loss: 2118.5586\n",
            "Epoch [520/1000], Loss: 2604.9731\n",
            "Epoch [520/1000], Loss: 2628.4592\n",
            "Epoch [520/1000], Loss: 1568.6472\n",
            "Epoch [530/1000], Loss: 4307.7251\n",
            "Epoch [530/1000], Loss: 3541.5911\n",
            "Epoch [530/1000], Loss: 2161.0574\n",
            "Epoch [530/1000], Loss: 2450.5159\n",
            "Epoch [530/1000], Loss: 2887.2629\n",
            "Epoch [530/1000], Loss: 2105.5723\n",
            "Epoch [530/1000], Loss: 2592.6714\n",
            "Epoch [530/1000], Loss: 8575.5801\n",
            "Epoch [540/1000], Loss: 3345.5967\n",
            "Epoch [540/1000], Loss: 2916.4448\n",
            "Epoch [540/1000], Loss: 3304.123\n",
            "Epoch [540/1000], Loss: 2390.6624\n",
            "Epoch [540/1000], Loss: 2314.6711\n",
            "Epoch [540/1000], Loss: 2769.4546\n",
            "Epoch [540/1000], Loss: 3239.0066\n",
            "Epoch [540/1000], Loss: 4645.3926\n",
            "Epoch [550/1000], Loss: 3916.3586\n",
            "Epoch [550/1000], Loss: 2808.1758\n",
            "Epoch [550/1000], Loss: 2806.5986\n",
            "Epoch [550/1000], Loss: 2610.6792\n",
            "Epoch [550/1000], Loss: 2722.4622\n",
            "Epoch [550/1000], Loss: 3067.3716\n",
            "Epoch [550/1000], Loss: 2344.5603\n",
            "Epoch [550/1000], Loss: 4637.9404\n",
            "Epoch [560/1000], Loss: 1852.7474\n",
            "Epoch [560/1000], Loss: 3172.3855\n",
            "Epoch [560/1000], Loss: 3377.4075\n",
            "Epoch [560/1000], Loss: 1995.342\n",
            "Epoch [560/1000], Loss: 4123.1655\n",
            "Epoch [560/1000], Loss: 2774.7646\n",
            "Epoch [560/1000], Loss: 3010.2937\n",
            "Epoch [560/1000], Loss: 4155.0234\n",
            "Epoch [570/1000], Loss: 3408.7881\n",
            "Epoch [570/1000], Loss: 2225.114\n",
            "Epoch [570/1000], Loss: 2646.0017\n",
            "Epoch [570/1000], Loss: 3891.5867\n",
            "Epoch [570/1000], Loss: 3019.1536\n",
            "Epoch [570/1000], Loss: 2798.9448\n",
            "Epoch [570/1000], Loss: 2308.5776\n",
            "Epoch [570/1000], Loss: 4141.5698\n",
            "Epoch [580/1000], Loss: 2520.3967\n",
            "Epoch [580/1000], Loss: 2827.8198\n",
            "Epoch [580/1000], Loss: 3412.9255\n",
            "Epoch [580/1000], Loss: 3476.2666\n",
            "Epoch [580/1000], Loss: 2345.1663\n",
            "Epoch [580/1000], Loss: 2333.1484\n",
            "Epoch [580/1000], Loss: 3582.6375\n",
            "Epoch [580/1000], Loss: 857.0952\n",
            "Epoch [590/1000], Loss: 1759.7712\n",
            "Epoch [590/1000], Loss: 3120.0149\n",
            "Epoch [590/1000], Loss: 2346.7952\n",
            "Epoch [590/1000], Loss: 2656.0813\n",
            "Epoch [590/1000], Loss: 2830.4656\n",
            "Epoch [590/1000], Loss: 3965.9319\n",
            "Epoch [590/1000], Loss: 3719.7429\n",
            "Epoch [590/1000], Loss: 2415.8418\n",
            "Epoch [600/1000], Loss: 2425.8059\n",
            "Epoch [600/1000], Loss: 2823.3\n",
            "Epoch [600/1000], Loss: 3055.0938\n",
            "Epoch [600/1000], Loss: 2568.6929\n",
            "Epoch [600/1000], Loss: 3447.6719\n",
            "Epoch [600/1000], Loss: 3655.8879\n",
            "Epoch [600/1000], Loss: 2422.8613\n",
            "Epoch [600/1000], Loss: 2402.0752\n",
            "Epoch [610/1000], Loss: 2329.5908\n",
            "Epoch [610/1000], Loss: 3423.5881\n",
            "Epoch [610/1000], Loss: 2961.1797\n",
            "Epoch [610/1000], Loss: 3548.8699\n",
            "Epoch [610/1000], Loss: 3037.2974\n",
            "Epoch [610/1000], Loss: 2552.9446\n",
            "Epoch [610/1000], Loss: 2365.8113\n",
            "Epoch [610/1000], Loss: 5328.2505\n",
            "Epoch [620/1000], Loss: 2638.1636\n",
            "Epoch [620/1000], Loss: 2289.2778\n",
            "Epoch [620/1000], Loss: 3152.0037\n",
            "Epoch [620/1000], Loss: 3216.3901\n",
            "Epoch [620/1000], Loss: 3496.2302\n",
            "Epoch [620/1000], Loss: 2463.6978\n",
            "Epoch [620/1000], Loss: 2973.6785\n",
            "Epoch [620/1000], Loss: 5134.5762\n",
            "Epoch [630/1000], Loss: 2358.9626\n",
            "Epoch [630/1000], Loss: 3394.1277\n",
            "Epoch [630/1000], Loss: 2905.7078\n",
            "Epoch [630/1000], Loss: 4288.8799\n",
            "Epoch [630/1000], Loss: 2469.519\n",
            "Epoch [630/1000], Loss: 2315.5769\n",
            "Epoch [630/1000], Loss: 2787.8328\n",
            "Epoch [630/1000], Loss: 230.7325\n",
            "Epoch [640/1000], Loss: 2884.1812\n",
            "Epoch [640/1000], Loss: 3100.6614\n",
            "Epoch [640/1000], Loss: 4151.3491\n",
            "Epoch [640/1000], Loss: 2270.2622\n",
            "Epoch [640/1000], Loss: 3071.8481\n",
            "Epoch [640/1000], Loss: 2818.3638\n",
            "Epoch [640/1000], Loss: 2185.9875\n",
            "Epoch [640/1000], Loss: 895.3229\n",
            "Epoch [650/1000], Loss: 3521.3774\n",
            "Epoch [650/1000], Loss: 2691.9536\n",
            "Epoch [650/1000], Loss: 2604.9062\n",
            "Epoch [650/1000], Loss: 2558.2937\n",
            "Epoch [650/1000], Loss: 2367.9221\n",
            "Epoch [650/1000], Loss: 3465.2275\n",
            "Epoch [650/1000], Loss: 3230.9778\n",
            "Epoch [650/1000], Loss: 1666.4666\n",
            "Epoch [660/1000], Loss: 3585.0281\n",
            "Epoch [660/1000], Loss: 2367.5186\n",
            "Epoch [660/1000], Loss: 2783.4783\n",
            "Epoch [660/1000], Loss: 3805.8018\n",
            "Epoch [660/1000], Loss: 1833.3147\n",
            "Epoch [660/1000], Loss: 2753.9924\n",
            "Epoch [660/1000], Loss: 3200.6125\n",
            "Epoch [660/1000], Loss: 3407.4897\n",
            "Epoch [670/1000], Loss: 2260.6506\n",
            "Epoch [670/1000], Loss: 2564.1428\n",
            "Epoch [670/1000], Loss: 2872.0479\n",
            "Epoch [670/1000], Loss: 3852.8755\n",
            "Epoch [670/1000], Loss: 2887.2913\n",
            "Epoch [670/1000], Loss: 3498.5269\n",
            "Epoch [670/1000], Loss: 2464.3369\n",
            "Epoch [670/1000], Loss: 2142.4468\n",
            "Epoch [680/1000], Loss: 3308.7937\n",
            "Epoch [680/1000], Loss: 2878.3274\n",
            "Epoch [680/1000], Loss: 2814.3408\n",
            "Epoch [680/1000], Loss: 3322.9968\n",
            "Epoch [680/1000], Loss: 3201.3547\n",
            "Epoch [680/1000], Loss: 2781.9507\n",
            "Epoch [680/1000], Loss: 2063.7256\n",
            "Epoch [680/1000], Loss: 2798.5771\n",
            "Epoch [690/1000], Loss: 2915.9006\n",
            "Epoch [690/1000], Loss: 3137.593\n",
            "Epoch [690/1000], Loss: 2872.0959\n",
            "Epoch [690/1000], Loss: 2630.1611\n",
            "Epoch [690/1000], Loss: 3062.7488\n",
            "Epoch [690/1000], Loss: 3319.0894\n",
            "Epoch [690/1000], Loss: 2498.312\n",
            "Epoch [690/1000], Loss: 1485.1592\n",
            "Epoch [700/1000], Loss: 2773.7634\n",
            "Epoch [700/1000], Loss: 2560.2571\n",
            "Epoch [700/1000], Loss: 2911.9312\n",
            "Epoch [700/1000], Loss: 3320.6316\n",
            "Epoch [700/1000], Loss: 3804.6274\n",
            "Epoch [700/1000], Loss: 2857.1528\n",
            "Epoch [700/1000], Loss: 2039.2356\n",
            "Epoch [700/1000], Loss: 4243.7007\n",
            "Epoch [710/1000], Loss: 3037.092\n",
            "Epoch [710/1000], Loss: 1637.835\n",
            "Epoch [710/1000], Loss: 2463.9878\n",
            "Epoch [710/1000], Loss: 3335.5027\n",
            "Epoch [710/1000], Loss: 3618.9006\n",
            "Epoch [710/1000], Loss: 3236.7649\n",
            "Epoch [710/1000], Loss: 2716.8953\n",
            "Epoch [710/1000], Loss: 7884.2578\n",
            "Epoch [720/1000], Loss: 2837.4392\n",
            "Epoch [720/1000], Loss: 3062.5442\n",
            "Epoch [720/1000], Loss: 2441.2639\n",
            "Epoch [720/1000], Loss: 2875.1611\n",
            "Epoch [720/1000], Loss: 2746.8706\n",
            "Epoch [720/1000], Loss: 2770.29\n",
            "Epoch [720/1000], Loss: 3521.6267\n",
            "Epoch [720/1000], Loss: 5049.3472\n",
            "Epoch [730/1000], Loss: 2434.7659\n",
            "Epoch [730/1000], Loss: 3415.2192\n",
            "Epoch [730/1000], Loss: 2911.5261\n",
            "Epoch [730/1000], Loss: 3175.8486\n",
            "Epoch [730/1000], Loss: 3639.9912\n",
            "Epoch [730/1000], Loss: 2804.9434\n",
            "Epoch [730/1000], Loss: 2085.3071\n",
            "Epoch [730/1000], Loss: 1139.6946\n",
            "Epoch [740/1000], Loss: 4023.2019\n",
            "Epoch [740/1000], Loss: 3001.8247\n",
            "Epoch [740/1000], Loss: 2098.7615\n",
            "Epoch [740/1000], Loss: 2635.4211\n",
            "Epoch [740/1000], Loss: 3007.625\n",
            "Epoch [740/1000], Loss: 2627.7087\n",
            "Epoch [740/1000], Loss: 3015.2727\n",
            "Epoch [740/1000], Loss: 1847.6758\n",
            "Epoch [750/1000], Loss: 3000.7861\n",
            "Epoch [750/1000], Loss: 3494.5049\n",
            "Epoch [750/1000], Loss: 2917.6384\n",
            "Epoch [750/1000], Loss: 2679.3716\n",
            "Epoch [750/1000], Loss: 2573.9578\n",
            "Epoch [750/1000], Loss: 1916.1949\n",
            "Epoch [750/1000], Loss: 3778.071\n",
            "Epoch [750/1000], Loss: 2637.6079\n",
            "Epoch [760/1000], Loss: 3492.8855\n",
            "Epoch [760/1000], Loss: 2869.74\n",
            "Epoch [760/1000], Loss: 2751.03\n",
            "Epoch [760/1000], Loss: 1921.8149\n",
            "Epoch [760/1000], Loss: 2032.5894\n",
            "Epoch [760/1000], Loss: 2978.3362\n",
            "Epoch [760/1000], Loss: 4375.0938\n",
            "Epoch [760/1000], Loss: 1572.0366\n",
            "Epoch [770/1000], Loss: 2440.3337\n",
            "Epoch [770/1000], Loss: 3167.7869\n",
            "Epoch [770/1000], Loss: 3120.4973\n",
            "Epoch [770/1000], Loss: 3001.8499\n",
            "Epoch [770/1000], Loss: 2667.636\n",
            "Epoch [770/1000], Loss: 3522.2422\n",
            "Epoch [770/1000], Loss: 2285.9556\n",
            "Epoch [770/1000], Loss: 5207.6211\n",
            "Epoch [780/1000], Loss: 2942.9224\n",
            "Epoch [780/1000], Loss: 2759.4858\n",
            "Epoch [780/1000], Loss: 2388.9253\n",
            "Epoch [780/1000], Loss: 2926.7749\n",
            "Epoch [780/1000], Loss: 3808.0911\n",
            "Epoch [780/1000], Loss: 2445.6106\n",
            "Epoch [780/1000], Loss: 3057.106\n",
            "Epoch [780/1000], Loss: 3063.9431\n",
            "Epoch [790/1000], Loss: 1959.1479\n",
            "Epoch [790/1000], Loss: 2425.5815\n",
            "Epoch [790/1000], Loss: 4297.2251\n",
            "Epoch [790/1000], Loss: 3003.0195\n",
            "Epoch [790/1000], Loss: 3012.0698\n",
            "Epoch [790/1000], Loss: 2429.7695\n",
            "Epoch [790/1000], Loss: 3182.6931\n",
            "Epoch [790/1000], Loss: 3640.0776\n",
            "Epoch [800/1000], Loss: 2149.0127\n",
            "Epoch [800/1000], Loss: 2903.4592\n",
            "Epoch [800/1000], Loss: 2843.6533\n",
            "Epoch [800/1000], Loss: 3431.0894\n",
            "Epoch [800/1000], Loss: 2916.8894\n",
            "Epoch [800/1000], Loss: 2993.0469\n",
            "Epoch [800/1000], Loss: 3169.0664\n",
            "Epoch [800/1000], Loss: 1878.293\n",
            "Epoch [810/1000], Loss: 1950.114\n",
            "Epoch [810/1000], Loss: 2829.1062\n",
            "Epoch [810/1000], Loss: 3680.7593\n",
            "Epoch [810/1000], Loss: 2901.9036\n",
            "Epoch [810/1000], Loss: 2840.4509\n",
            "Epoch [810/1000], Loss: 3080.5229\n",
            "Epoch [810/1000], Loss: 2967.4329\n",
            "Epoch [810/1000], Loss: 4499.8984\n",
            "Epoch [820/1000], Loss: 3083.9778\n",
            "Epoch [820/1000], Loss: 2902.7749\n",
            "Epoch [820/1000], Loss: 3083.0\n",
            "Epoch [820/1000], Loss: 2863.7991\n",
            "Epoch [820/1000], Loss: 2589.5366\n",
            "Epoch [820/1000], Loss: 2748.0581\n",
            "Epoch [820/1000], Loss: 2902.1094\n",
            "Epoch [820/1000], Loss: 5832.8379\n",
            "Epoch [830/1000], Loss: 2455.8198\n",
            "Epoch [830/1000], Loss: 2955.9375\n",
            "Epoch [830/1000], Loss: 2830.8599\n",
            "Epoch [830/1000], Loss: 2737.4824\n",
            "Epoch [830/1000], Loss: 2293.1072\n",
            "Epoch [830/1000], Loss: 4149.9761\n",
            "Epoch [830/1000], Loss: 3001.6274\n",
            "Epoch [830/1000], Loss: 2049.3022\n",
            "Epoch [840/1000], Loss: 2411.1479\n",
            "Epoch [840/1000], Loss: 3062.0891\n",
            "Epoch [840/1000], Loss: 2005.1431\n",
            "Epoch [840/1000], Loss: 2754.6721\n",
            "Epoch [840/1000], Loss: 4401.7788\n",
            "Epoch [840/1000], Loss: 3338.218\n",
            "Epoch [840/1000], Loss: 2254.2952\n",
            "Epoch [840/1000], Loss: 4980.8569\n",
            "Epoch [850/1000], Loss: 3798.8352\n",
            "Epoch [850/1000], Loss: 3152.968\n",
            "Epoch [850/1000], Loss: 2696.2681\n",
            "Epoch [850/1000], Loss: 2525.3699\n",
            "Epoch [850/1000], Loss: 3571.8374\n",
            "Epoch [850/1000], Loss: 2899.9836\n",
            "Epoch [850/1000], Loss: 1691.9454\n",
            "Epoch [850/1000], Loss: 2908.8003\n",
            "Epoch [860/1000], Loss: 2608.4766\n",
            "Epoch [860/1000], Loss: 3007.0457\n",
            "Epoch [860/1000], Loss: 3321.834\n",
            "Epoch [860/1000], Loss: 2431.9644\n",
            "Epoch [860/1000], Loss: 2166.9871\n",
            "Epoch [860/1000], Loss: 3057.7175\n",
            "Epoch [860/1000], Loss: 3861.2395\n",
            "Epoch [860/1000], Loss: 907.5491\n",
            "Epoch [870/1000], Loss: 2456.8352\n",
            "Epoch [870/1000], Loss: 1952.4907\n",
            "Epoch [870/1000], Loss: 2676.4507\n",
            "Epoch [870/1000], Loss: 3907.4702\n",
            "Epoch [870/1000], Loss: 3143.6692\n",
            "Epoch [870/1000], Loss: 2570.1421\n",
            "Epoch [870/1000], Loss: 3760.9487\n",
            "Epoch [870/1000], Loss: 685.9045\n",
            "Epoch [880/1000], Loss: 3123.5781\n",
            "Epoch [880/1000], Loss: 3255.3833\n",
            "Epoch [880/1000], Loss: 2293.5244\n",
            "Epoch [880/1000], Loss: 3409.6621\n",
            "Epoch [880/1000], Loss: 2753.4429\n",
            "Epoch [880/1000], Loss: 3311.3154\n",
            "Epoch [880/1000], Loss: 2292.0869\n",
            "Epoch [880/1000], Loss: 975.3854\n",
            "Epoch [890/1000], Loss: 3178.2612\n",
            "Epoch [890/1000], Loss: 3000.6541\n",
            "Epoch [890/1000], Loss: 2804.4397\n",
            "Epoch [890/1000], Loss: 2287.4263\n",
            "Epoch [890/1000], Loss: 2483.6689\n",
            "Epoch [890/1000], Loss: 3173.2512\n",
            "Epoch [890/1000], Loss: 3315.1077\n",
            "Epoch [890/1000], Loss: 4510.6333\n",
            "Epoch [900/1000], Loss: 3026.6521\n",
            "Epoch [900/1000], Loss: 2773.7632\n",
            "Epoch [900/1000], Loss: 4542.293\n",
            "Epoch [900/1000], Loss: 2413.6687\n",
            "Epoch [900/1000], Loss: 2380.5381\n",
            "Epoch [900/1000], Loss: 1977.8997\n",
            "Epoch [900/1000], Loss: 3196.1306\n",
            "Epoch [900/1000], Loss: 3250.5806\n",
            "Epoch [910/1000], Loss: 2799.4045\n",
            "Epoch [910/1000], Loss: 2849.0059\n",
            "Epoch [910/1000], Loss: 4090.0247\n",
            "Epoch [910/1000], Loss: 2748.2332\n",
            "Epoch [910/1000], Loss: 2383.4363\n",
            "Epoch [910/1000], Loss: 3076.1758\n",
            "Epoch [910/1000], Loss: 2403.467\n",
            "Epoch [910/1000], Loss: 2551.8818\n",
            "Epoch [920/1000], Loss: 4125.959\n",
            "Epoch [920/1000], Loss: 2490.4993\n",
            "Epoch [920/1000], Loss: 3550.3889\n",
            "Epoch [920/1000], Loss: 3593.1006\n",
            "Epoch [920/1000], Loss: 2303.4919\n",
            "Epoch [920/1000], Loss: 2253.0359\n",
            "Epoch [920/1000], Loss: 2111.5933\n",
            "Epoch [920/1000], Loss: 1311.1429\n",
            "Epoch [930/1000], Loss: 2392.76\n",
            "Epoch [930/1000], Loss: 2432.251\n",
            "Epoch [930/1000], Loss: 2489.875\n",
            "Epoch [930/1000], Loss: 3228.856\n",
            "Epoch [930/1000], Loss: 3011.5542\n",
            "Epoch [930/1000], Loss: 3510.0684\n",
            "Epoch [930/1000], Loss: 3024.5972\n",
            "Epoch [930/1000], Loss: 6816.6602\n",
            "Epoch [940/1000], Loss: 3269.7546\n",
            "Epoch [940/1000], Loss: 2706.8496\n",
            "Epoch [940/1000], Loss: 2679.9299\n",
            "Epoch [940/1000], Loss: 3379.3455\n",
            "Epoch [940/1000], Loss: 3311.1187\n",
            "Epoch [940/1000], Loss: 2233.377\n",
            "Epoch [940/1000], Loss: 2855.593\n",
            "Epoch [940/1000], Loss: 952.4476\n",
            "Epoch [950/1000], Loss: 2468.813\n",
            "Epoch [950/1000], Loss: 3251.8169\n",
            "Epoch [950/1000], Loss: 2424.2505\n",
            "Epoch [950/1000], Loss: 2881.7034\n",
            "Epoch [950/1000], Loss: 3373.1511\n",
            "Epoch [950/1000], Loss: 2702.9624\n",
            "Epoch [950/1000], Loss: 3285.0325\n",
            "Epoch [950/1000], Loss: 1802.3936\n",
            "Epoch [960/1000], Loss: 2428.5144\n",
            "Epoch [960/1000], Loss: 2541.2617\n",
            "Epoch [960/1000], Loss: 2664.1489\n",
            "Epoch [960/1000], Loss: 2641.2156\n",
            "Epoch [960/1000], Loss: 3450.4812\n",
            "Epoch [960/1000], Loss: 3426.7437\n",
            "Epoch [960/1000], Loss: 3252.8181\n",
            "Epoch [960/1000], Loss: 1651.3905\n",
            "Epoch [970/1000], Loss: 3021.4019\n",
            "Epoch [970/1000], Loss: 3530.0774\n",
            "Epoch [970/1000], Loss: 2772.4556\n",
            "Epoch [970/1000], Loss: 2022.5438\n",
            "Epoch [970/1000], Loss: 3100.9961\n",
            "Epoch [970/1000], Loss: 3451.8765\n",
            "Epoch [970/1000], Loss: 2473.3206\n",
            "Epoch [970/1000], Loss: 2128.7217\n",
            "Epoch [980/1000], Loss: 3056.5635\n",
            "Epoch [980/1000], Loss: 3482.447\n",
            "Epoch [980/1000], Loss: 2337.2922\n",
            "Epoch [980/1000], Loss: 2625.0178\n",
            "Epoch [980/1000], Loss: 2758.0493\n",
            "Epoch [980/1000], Loss: 2753.324\n",
            "Epoch [980/1000], Loss: 3011.9548\n",
            "Epoch [980/1000], Loss: 7903.1328\n",
            "Epoch [990/1000], Loss: 3151.8394\n",
            "Epoch [990/1000], Loss: 2467.894\n",
            "Epoch [990/1000], Loss: 3357.51\n",
            "Epoch [990/1000], Loss: 3193.738\n",
            "Epoch [990/1000], Loss: 2592.1453\n",
            "Epoch [990/1000], Loss: 2931.4292\n",
            "Epoch [990/1000], Loss: 2699.2\n",
            "Epoch [990/1000], Loss: 1719.1689\n",
            "Epoch [1000/1000], Loss: 3364.0374\n",
            "Epoch [1000/1000], Loss: 2986.9614\n",
            "Epoch [1000/1000], Loss: 2221.5947\n",
            "Epoch [1000/1000], Loss: 2791.0791\n",
            "Epoch [1000/1000], Loss: 3190.0449\n",
            "Epoch [1000/1000], Loss: 3223.2104\n",
            "Epoch [1000/1000], Loss: 2524.1936\n",
            "Epoch [1000/1000], Loss: 3176.9253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000  # 1000 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in train_loader:  # Loop through each mini-batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        model.train()  # Set the model to training mode\n",
        "        optimiser.zero_grad()  # Reset the gradients\n",
        "        outputs = model(inputs)  # Create outputs\n",
        "        loss = criterion(outputs, targets)  # Calculate loss\n",
        "        loss.backward()  # Backpropagate the loss\n",
        "        optimiser.step()  # Update model parameters\n",
        "\n",
        "    # Move this print statement outside the inner loop\n",
        "    if (epoch + 1) % 10 == 0:  # Print every 10 epochs\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {round(loss.item(), 4)}')\n",
        "\n",
        "# I think this is what we want to see, rather than a loss result for every mini batch."
      ],
      "metadata": {
        "id": "1v8uHxFnr3ZG",
        "outputId": "3526c5ea-8642-49c9-9f6f-2945c2fc33d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6434.3159\n",
            "Epoch [20/1000], Loss: 607.5897\n",
            "Epoch [30/1000], Loss: 1833.1444\n",
            "Epoch [40/1000], Loss: 4755.5967\n",
            "Epoch [50/1000], Loss: 7128.689\n",
            "Epoch [60/1000], Loss: 2974.7104\n",
            "Epoch [70/1000], Loss: 4553.4272\n",
            "Epoch [80/1000], Loss: 5294.9897\n",
            "Epoch [90/1000], Loss: 1623.6699\n",
            "Epoch [100/1000], Loss: 1183.1851\n",
            "Epoch [110/1000], Loss: 4874.0273\n",
            "Epoch [120/1000], Loss: 11091.3184\n",
            "Epoch [130/1000], Loss: 1224.8403\n",
            "Epoch [140/1000], Loss: 608.1396\n",
            "Epoch [150/1000], Loss: 2473.0676\n",
            "Epoch [160/1000], Loss: 6082.0186\n",
            "Epoch [170/1000], Loss: 4935.4443\n",
            "Epoch [180/1000], Loss: 4512.9033\n",
            "Epoch [190/1000], Loss: 926.8905\n",
            "Epoch [200/1000], Loss: 2647.1196\n",
            "Epoch [210/1000], Loss: 5413.8213\n",
            "Epoch [220/1000], Loss: 562.4753\n",
            "Epoch [230/1000], Loss: 1737.8423\n",
            "Epoch [240/1000], Loss: 4754.376\n",
            "Epoch [250/1000], Loss: 2340.3015\n",
            "Epoch [260/1000], Loss: 5145.7295\n",
            "Epoch [270/1000], Loss: 1918.2134\n",
            "Epoch [280/1000], Loss: 4178.7485\n",
            "Epoch [290/1000], Loss: 1381.9252\n",
            "Epoch [300/1000], Loss: 1955.9756\n",
            "Epoch [310/1000], Loss: 664.3322\n",
            "Epoch [320/1000], Loss: 5053.9219\n",
            "Epoch [330/1000], Loss: 3786.7671\n",
            "Epoch [340/1000], Loss: 3351.8257\n",
            "Epoch [350/1000], Loss: 784.5029\n",
            "Epoch [360/1000], Loss: 2801.958\n",
            "Epoch [370/1000], Loss: 4335.9775\n",
            "Epoch [380/1000], Loss: 6120.3984\n",
            "Epoch [390/1000], Loss: 2566.4231\n",
            "Epoch [400/1000], Loss: 3638.8628\n",
            "Epoch [410/1000], Loss: 442.7852\n",
            "Epoch [420/1000], Loss: 1836.9135\n",
            "Epoch [430/1000], Loss: 8721.0391\n",
            "Epoch [440/1000], Loss: 3018.3992\n",
            "Epoch [450/1000], Loss: 946.3608\n",
            "Epoch [460/1000], Loss: 6996.0854\n",
            "Epoch [470/1000], Loss: 518.796\n",
            "Epoch [480/1000], Loss: 6730.3862\n",
            "Epoch [490/1000], Loss: 299.5525\n",
            "Epoch [500/1000], Loss: 589.4824\n",
            "Epoch [510/1000], Loss: 2646.3652\n",
            "Epoch [520/1000], Loss: 1322.7473\n",
            "Epoch [530/1000], Loss: 2804.9521\n",
            "Epoch [540/1000], Loss: 2027.2078\n",
            "Epoch [550/1000], Loss: 1637.8285\n",
            "Epoch [560/1000], Loss: 2878.5195\n",
            "Epoch [570/1000], Loss: 3088.8013\n",
            "Epoch [580/1000], Loss: 6769.3101\n",
            "Epoch [590/1000], Loss: 3500.3945\n",
            "Epoch [600/1000], Loss: 246.5486\n",
            "Epoch [610/1000], Loss: 2163.8042\n",
            "Epoch [620/1000], Loss: 1896.2479\n",
            "Epoch [630/1000], Loss: 2237.468\n",
            "Epoch [640/1000], Loss: 5107.752\n",
            "Epoch [650/1000], Loss: 2052.1704\n",
            "Epoch [660/1000], Loss: 2090.5435\n",
            "Epoch [670/1000], Loss: 424.3593\n",
            "Epoch [680/1000], Loss: 5316.8213\n",
            "Epoch [690/1000], Loss: 2523.7197\n",
            "Epoch [700/1000], Loss: 4386.5049\n",
            "Epoch [710/1000], Loss: 6174.4585\n",
            "Epoch [720/1000], Loss: 1235.2422\n",
            "Epoch [730/1000], Loss: 2382.8931\n",
            "Epoch [740/1000], Loss: 902.4192\n",
            "Epoch [750/1000], Loss: 4048.7856\n",
            "Epoch [760/1000], Loss: 4110.603\n",
            "Epoch [770/1000], Loss: 1191.7666\n",
            "Epoch [780/1000], Loss: 1018.543\n",
            "Epoch [790/1000], Loss: 5356.3325\n",
            "Epoch [800/1000], Loss: 1393.7244\n",
            "Epoch [810/1000], Loss: 2391.7275\n",
            "Epoch [820/1000], Loss: 2919.4607\n",
            "Epoch [830/1000], Loss: 2491.5737\n",
            "Epoch [840/1000], Loss: 2585.8604\n",
            "Epoch [850/1000], Loss: 1808.3142\n",
            "Epoch [860/1000], Loss: 2413.8306\n",
            "Epoch [870/1000], Loss: 237.1476\n",
            "Epoch [880/1000], Loss: 2738.7214\n",
            "Epoch [890/1000], Loss: 1519.1453\n",
            "Epoch [900/1000], Loss: 3738.1851\n",
            "Epoch [910/1000], Loss: 742.4578\n",
            "Epoch [920/1000], Loss: 3089.2046\n",
            "Epoch [930/1000], Loss: 5435.2007\n",
            "Epoch [940/1000], Loss: 1107.4052\n",
            "Epoch [950/1000], Loss: 1774.1895\n",
            "Epoch [960/1000], Loss: 2220.1272\n",
            "Epoch [970/1000], Loss: 1325.7754\n",
            "Epoch [980/1000], Loss: 4833.562\n",
            "Epoch [990/1000], Loss: 767.4838\n",
            "Epoch [1000/1000], Loss: 503.6649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "id": "B9PoiIJCpesM",
        "outputId": "2636e762-7277-4a94-e048-7eafd80d47ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2872.356201171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "id": "FzXnHFiepgXi",
        "outputId": "a603b5ad-7a53-4881-a56b-413ba875ee40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   139.645920   219.0\n",
              "1   181.327118    70.0\n",
              "2   139.051834   202.0\n",
              "3   294.347931   230.0\n",
              "4   120.210449   111.0\n",
              "..         ...     ...\n",
              "84  111.432915   153.0\n",
              "85   85.554550    98.0\n",
              "86   80.706291    37.0\n",
              "87   62.741089    63.0\n",
              "88  163.021942   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3de33360-2524-4f15-9c4a-58fcbca979f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>139.645920</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>181.327118</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>139.051834</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>294.347931</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>120.210449</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>111.432915</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>85.554550</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>80.706291</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>62.741089</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>163.021942</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3de33360-2524-4f15-9c4a-58fcbca979f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3de33360-2524-4f15-9c4a-58fcbca979f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3de33360-2524-4f15-9c4a-58fcbca979f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-aa4b9963-4528-4833-8656-a8cefbaf57c0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa4b9963-4528-4833-8656-a8cefbaf57c0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-aa4b9963-4528-4833-8656-a8cefbaf57c0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_19f98cff-0049-451b-9083-bc6af791bf20\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_19f98cff-0049-451b-9083-bc6af791bf20 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          175.95849609375,\n          107.92659759521484,\n          180.11380004882812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the results are much better."
      ],
      "metadata": {
        "id": "EctJrQeEpw3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EXERCISE #2"
      ],
      "metadata": {
        "id": "pOAwqvY2rQ9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self, num_neurons):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(10, num_neurons),  # First layer with specified number of neurons\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_neurons, num_neurons),  # Second layer with specified number of neurons\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_neurons, 1)  # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Function to train and evaluate the model\n",
        "def train_and_evaluate(num_neurons, X_train, y_train, X_val, y_val, epochs=100):\n",
        "    model = DiabetesModel(num_neurons)\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        outputs = model(X_train)  # Forward pass\n",
        "        loss = criterion(outputs, y_train)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, y_val)  # Validation loss\n",
        "\n",
        "    return val_loss.item()  # Return the validation loss value\n",
        "\n",
        "# Example dataset\n",
        "# Assume X and y are your dataset features and labels\n",
        "# X = ...  # Your input data (shape: [n_samples, 10])\n",
        "# y = ...  # Your targets (shape: [n_samples, 1])\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'num_neurons': [5, 10, 20]  # Try different numbers of neurons\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Grid Search\n",
        "for num_neurons in param_grid['num_neurons']:\n",
        "    print(f'Training with {num_neurons} neurons...')\n",
        "    val_loss = train_and_evaluate(num_neurons, X_train, y_train, X_val, y_val)\n",
        "    results.append((num_neurons, val_loss))\n",
        "    print(f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "# Display results\n",
        "for num_neurons, val_loss in results:\n",
        "    print(f'Neuron count: {num_neurons}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Identify and print the best configuration\n",
        "best_configuration = min(results, key=lambda x: x[1])  # Get configuration with the lowest loss\n",
        "best_neurons, best_val_loss = best_configuration\n",
        "print(f'Best Configuration: {best_neurons} neurons with Validation Loss: {best_val_loss:.4f}')"
      ],
      "metadata": {
        "id": "Hj6_sNRmpwYO",
        "outputId": "79086022-1536-4c17-913a-c4031ec46e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 5 neurons...\n",
            "Validation Loss: 20826.4043\n",
            "Training with 10 neurons...\n",
            "Validation Loss: 4761.3345\n",
            "Training with 20 neurons...\n",
            "Validation Loss: 3326.6748\n",
            "Neuron count: 5, Validation Loss: 20826.4043\n",
            "Neuron count: 10, Validation Loss: 4761.3345\n",
            "Neuron count: 20, Validation Loss: 3326.6748\n",
            "\n",
            "\n",
            "Best Configuration: 20 neurons with Validation Loss: 3326.6748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self, num_neurons):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(10, num_neurons),  # First layer with specified number of neurons\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_neurons, num_neurons),  # Second layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(num_neurons, 1)  # Output layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel(num_neurons=10)  # Example number of neurons\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)  # Optimizer"
      ],
      "metadata": {
        "id": "faGropv8tIp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000  # 1000 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in train_loader:  # Loop through each mini-batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        model.to(device)\n",
        "\n",
        "        model.train()  # Set the model to training mode\n",
        "        optimiser.zero_grad()  # Reset the gradients\n",
        "        outputs = model(inputs)  # Create outputs\n",
        "        loss = criterion(outputs, targets)  # Calculate loss\n",
        "        loss.backward()  # Backpropagate the loss\n",
        "        optimiser.step()  # Update model parameters\n",
        "\n",
        "    # Move this print statement outside the inner loop\n",
        "    if (epoch + 1) % 10 == 0:  # Print every 10 epochs\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "id": "Z8NXnCEgtR3I",
        "outputId": "4a2345ae-790c-4671-8be4-e5d03aa878ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 29092.8203\n",
            "Epoch [20/1000], Loss: 20807.7227\n",
            "Epoch [30/1000], Loss: 69729.6641\n",
            "Epoch [40/1000], Loss: 23701.7246\n",
            "Epoch [50/1000], Loss: 36085.6719\n",
            "Epoch [60/1000], Loss: 44277.4961\n",
            "Epoch [70/1000], Loss: 4569.9287\n",
            "Epoch [80/1000], Loss: 17589.1035\n",
            "Epoch [90/1000], Loss: 29442.3828\n",
            "Epoch [100/1000], Loss: 2877.0623\n",
            "Epoch [110/1000], Loss: 15457.1543\n",
            "Epoch [120/1000], Loss: 11190.1514\n",
            "Epoch [130/1000], Loss: 4006.5928\n",
            "Epoch [140/1000], Loss: 1163.1003\n",
            "Epoch [150/1000], Loss: 3519.437\n",
            "Epoch [160/1000], Loss: 1173.3394\n",
            "Epoch [170/1000], Loss: 2044.0442\n",
            "Epoch [180/1000], Loss: 1468.3828\n",
            "Epoch [190/1000], Loss: 5716.2617\n",
            "Epoch [200/1000], Loss: 5538.5674\n",
            "Epoch [210/1000], Loss: 1888.0559\n",
            "Epoch [220/1000], Loss: 3542.5496\n",
            "Epoch [230/1000], Loss: 4783.582\n",
            "Epoch [240/1000], Loss: 2970.1084\n",
            "Epoch [250/1000], Loss: 6371.5449\n",
            "Epoch [260/1000], Loss: 4765.249\n",
            "Epoch [270/1000], Loss: 3613.3901\n",
            "Epoch [280/1000], Loss: 1409.1812\n",
            "Epoch [290/1000], Loss: 6490.644\n",
            "Epoch [300/1000], Loss: 3667.2109\n",
            "Epoch [310/1000], Loss: 1629.1833\n",
            "Epoch [320/1000], Loss: 5330.2563\n",
            "Epoch [330/1000], Loss: 2721.9668\n",
            "Epoch [340/1000], Loss: 8441.1191\n",
            "Epoch [350/1000], Loss: 3609.9741\n",
            "Epoch [360/1000], Loss: 3601.7361\n",
            "Epoch [370/1000], Loss: 5021.6597\n",
            "Epoch [380/1000], Loss: 4545.9858\n",
            "Epoch [390/1000], Loss: 831.8749\n",
            "Epoch [400/1000], Loss: 888.9059\n",
            "Epoch [410/1000], Loss: 3784.3486\n",
            "Epoch [420/1000], Loss: 3573.2578\n",
            "Epoch [430/1000], Loss: 4581.1851\n",
            "Epoch [440/1000], Loss: 347.6883\n",
            "Epoch [450/1000], Loss: 2086.4856\n",
            "Epoch [460/1000], Loss: 6492.9639\n",
            "Epoch [470/1000], Loss: 1817.6792\n",
            "Epoch [480/1000], Loss: 2240.0002\n",
            "Epoch [490/1000], Loss: 2691.8848\n",
            "Epoch [500/1000], Loss: 2263.3223\n",
            "Epoch [510/1000], Loss: 4341.0938\n",
            "Epoch [520/1000], Loss: 6664.1709\n",
            "Epoch [530/1000], Loss: 848.5617\n",
            "Epoch [540/1000], Loss: 7071.7158\n",
            "Epoch [550/1000], Loss: 1297.9948\n",
            "Epoch [560/1000], Loss: 2668.3054\n",
            "Epoch [570/1000], Loss: 3700.7549\n",
            "Epoch [580/1000], Loss: 2829.3132\n",
            "Epoch [590/1000], Loss: 4767.0596\n",
            "Epoch [600/1000], Loss: 1124.837\n",
            "Epoch [610/1000], Loss: 3274.7429\n",
            "Epoch [620/1000], Loss: 2712.2012\n",
            "Epoch [630/1000], Loss: 6258.0205\n",
            "Epoch [640/1000], Loss: 6199.3721\n",
            "Epoch [650/1000], Loss: 1959.4292\n",
            "Epoch [660/1000], Loss: 3549.9988\n",
            "Epoch [670/1000], Loss: 1999.1111\n",
            "Epoch [680/1000], Loss: 6111.4678\n",
            "Epoch [690/1000], Loss: 3205.6001\n",
            "Epoch [700/1000], Loss: 4669.6514\n",
            "Epoch [710/1000], Loss: 1324.375\n",
            "Epoch [720/1000], Loss: 198.8416\n",
            "Epoch [730/1000], Loss: 836.3996\n",
            "Epoch [740/1000], Loss: 164.7405\n",
            "Epoch [750/1000], Loss: 291.5519\n",
            "Epoch [760/1000], Loss: 4940.5508\n",
            "Epoch [770/1000], Loss: 4770.103\n",
            "Epoch [780/1000], Loss: 1630.891\n",
            "Epoch [790/1000], Loss: 4574.7651\n",
            "Epoch [800/1000], Loss: 3319.6182\n",
            "Epoch [810/1000], Loss: 4672.4521\n",
            "Epoch [820/1000], Loss: 5087.4111\n",
            "Epoch [830/1000], Loss: 6423.9697\n",
            "Epoch [840/1000], Loss: 1158.458\n",
            "Epoch [850/1000], Loss: 2514.3198\n",
            "Epoch [860/1000], Loss: 3559.9233\n",
            "Epoch [870/1000], Loss: 1343.819\n",
            "Epoch [880/1000], Loss: 1603.14\n",
            "Epoch [890/1000], Loss: 1066.937\n",
            "Epoch [900/1000], Loss: 2810.6658\n",
            "Epoch [910/1000], Loss: 2687.1528\n",
            "Epoch [920/1000], Loss: 2533.5176\n",
            "Epoch [930/1000], Loss: 1221.1444\n",
            "Epoch [940/1000], Loss: 613.2404\n",
            "Epoch [950/1000], Loss: 627.2076\n",
            "Epoch [960/1000], Loss: 5898.5635\n",
            "Epoch [970/1000], Loss: 5293.4976\n",
            "Epoch [980/1000], Loss: 4211.3037\n",
            "Epoch [990/1000], Loss: 749.9683\n",
            "Epoch [1000/1000], Loss: 2156.7529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "id": "I-lxVBeetT-S",
        "outputId": "11cfcb79-111a-4e99-9b57-97244f1a50a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2851.9542236328125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "id": "YxQfCV9WuPHz",
        "outputId": "26aa2007-cf44-4f8b-aa42-f82c5982cca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   144.537506   219.0\n",
              "1   178.024811    70.0\n",
              "2   140.150604   202.0\n",
              "3   296.237030   230.0\n",
              "4   125.762863   111.0\n",
              "..         ...     ...\n",
              "84  115.660217   153.0\n",
              "85   88.124664    98.0\n",
              "86   77.564072    37.0\n",
              "87   65.011871    63.0\n",
              "88  156.845215   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8e68480-9175-420a-a647-064db6b3f1cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>144.537506</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>178.024811</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>140.150604</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>296.237030</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>125.762863</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>115.660217</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>88.124664</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>77.564072</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>65.011871</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>156.845215</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8e68480-9175-420a-a647-064db6b3f1cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8e68480-9175-420a-a647-064db6b3f1cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8e68480-9175-420a-a647-064db6b3f1cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-23fe71ae-e49b-4b3f-abf5-aefe0908e9f8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23fe71ae-e49b-4b3f-abf5-aefe0908e9f8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-23fe71ae-e49b-4b3f-abf5-aefe0908e9f8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_159fa6c5-d50b-4ff1-802e-c5ae940e762e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_159fa6c5-d50b-4ff1-802e-c5ae940e762e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          177.1156463623047,\n          111.11229705810547,\n          178.79177856445312\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}